# -*- coding: utf-8 -*-
"""Graphs and sensitivity analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WCGUS-tzZbH5pU2cNVHRtZ4wJCAQr0ja
"""

!pip install dggrid4py
!pip install rasterio
!pip install folium matplotlib mapclassify

import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import connected_components

from dggrid4py import DGGRIDv7
from dggrid4py import igeo7

import importlib
from importlib import reload
import igeo7_ext
reload(igeo7_ext)

from igeo7_ext import dggrid_igeo7_q2di_from_cellids

import pandas as pd
import numpy as np

from igeo7_ext import (
    to_parent_series,
    dggrid_igeo7_q2di_from_cellids,
    dggrid_igeo7_grid_cell_polygons_from_cellids,
    dggrid_get_res,
    download_executable
)
import tqdm

import ast
from collections import defaultdict

from pathlib import Path
import os
import io
import sys
import copy
import stat
import shutil
import math
import json

import requests
import numpy as np
import pandas as pd

import pyproj
from pyproj import CRS
import geopandas as gpd
from shapely.geometry import box, mapping, shape
from shapely.geometry import Point, Polygon
from shapely.ops import transform

import rasterio
from rasterio.windows import Window
from affine import Affine

import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds

from tqdm import tqdm



# we need the DGGRID executable
# For a `micromamba` / `miniconda` enviroment we can install via conda/micromamba (conda install -c conda-forge dggrid)
# if not in a conda environment, skip to next cell

dggrid_exec = 'dggrid'

# if installed with conda/micromamba or compiled locally, we can ask the system for the location
if not os.path.isfile(dggrid_exec) and not os.access(dggrid_exec, os.X_OK):
    if shutil.which("dggrid83"):
        dggrid_exec = shutil.which("dggrid83")
    elif shutil.which("dggrid8"):
        dggrid_exec = shutil.which("dggrid8")
    elif shutil.which("dggrid"):
        dggrid_exec = shutil.which("dggrid")
    else:
        print("No dggrid executable found in path")

    if os.path.isfile(dggrid_exec) and os.access(dggrid_exec, os.X_OK):
        print(f"A DGGRID executable is available at: {dggrid_exec}")
    else:
        print("No usable dggrid executable found in path")
else:
    print(f"Executable already available: {dggrid_exec}")

# dggrid_exec = None

# we need to download DGGRID executable (only works on Linux, colab should work)
# For a `micromamba` / `miniconda` enviroment we can install via conda/micromamba (conda install -c conda-forge dggrid)

# Use the function to download the executable
url = "https://storage.googleapis.com/geo-assets/dggs-dev/dggrid"
dggrid_exec = '/Users/akmoch/dev/build/geolynx/ujaval_climate_trends/bin/dggrid-linux-x64'

from igeo7_ext import download_executable
# def download_executable(url, folder="./"):

if not os.path.isfile(dggrid_exec) and not os.access(dggrid_exec, os.X_OK):
    dggrid_exec = download_executable(url, "./bin")
    print(f"Downloaded and made executable: {dggrid_exec}")
else:
    print(f"Executable already available: {dggrid_exec}")

dggrid_exec

from dggrid4py import DGGRIDv7
from dggrid4py import igeo7

if not os.path.isfile(dggrid_exec) or not os.access(dggrid_exec, os.X_OK):
    raise ValueError(f"dggrid executable not found or not executable: {dggrid_exec}")

working_dir=os.curdir
capture_logs=True
silent=True
# if self-compiled or installed with conda, we can set performance options `tmp_geo_out_legacy=False`and `has_gdal=True`
tmp_geo_out_legacy=False
has_gdal=True
debug=False

dggrid_instance = DGGRIDv7(
    executable=dggrid_exec, working_dir=working_dir, capture_logs=capture_logs, silent=silent, tmp_geo_out_legacy=tmp_geo_out_legacy, has_gdal=has_gdal, debug=debug
)

def calculate_class_proportions(df):
    cell_area_lookup = cell_area_df['average_hexagon_area_m2'].to_dict()

    flat_counts = df.groupby(['resolution', 'landuse_class_flat']).size().reset_index(name='count_flat')
    hier_counts = df.groupby(['resolution', 'landuse_class_hierarchical']).size().reset_index(name='count_hier')

    merged = pd.merge(
        flat_counts,
        hier_counts,
        left_on=['resolution', 'landuse_class_flat'],
        right_on=['resolution', 'landuse_class_hierarchical'],
        how='outer'
    )

    merged['landuse_class_flat'] = merged['landuse_class_flat'].fillna(merged['landuse_class_hierarchical'])
    merged['landuse_class_hierarchical'] = merged['landuse_class_hierarchical'].fillna(merged['landuse_class_flat'])
    merged['count_flat'] = merged['count_flat'].fillna(0)
    merged['count_hier'] = merged['count_hier'].fillna(0)

    def calc_proportions(row):
        res = int(row['resolution'])
        cell_area = cell_area_lookup.get(res, 0)
        total_cells = (df['resolution'] == res).sum()
        total_area = total_cells * cell_area
        return pd.Series({
            'flat': row['count_flat'] * cell_area * 100 / total_area,
            'hier': row['count_hier'] * cell_area * 100 / total_area
        })

    proportions = merged.apply(calc_proportions, axis=1)
    merged['flat_proportion'] = proportions['flat']
    merged['hier_proportion'] = proportions['hier']

    merged = merged[['resolution', 'landuse_class_flat', 'flat_proportion', 'hier_proportion']].round(2)
    merged = merged.rename(columns={'landuse_class_flat': 'code'})

    return merged

calculate_class_proportions(df)

"""# **Patch Density**"""

def calculate_patch_density(df):
    """
    Calculate patch density (patches per hectare) for each land use class and resolution.
    Returns:
        DataFrame with columns: resolution, code (landuse_class), patch_density_flat, patch_density_hier
    """
    from scipy.sparse import csr_matrix
    from scipy.sparse.csgraph import connected_components

    def compute_density(sub_df, class_column, res):
        indexed_df = sub_df[['I', 'J', class_column]].copy()
        indexed_df['Z7_STRING'] = sub_df.index
        indexed_df = indexed_df.set_index(['I', 'J'])

        z7_dict = indexed_df['Z7_STRING'].to_dict()
        density_records = []

        for majority_class in indexed_df[class_column].dropna().unique():
            class_df = indexed_df[indexed_df[class_column] == majority_class]
            if class_df.empty:
                continue

            name_to_index = {name: idx for idx, name in enumerate(class_df['Z7_STRING'])}

            if len(name_to_index) == 1:
                # Only one cell: one patch by default
                num_patches = 1
            else:
                row_indices, col_indices = [], []
                for t in class_df.itertuples():
                    cell_id = z7_dict.get(t.Index)
                    neighbors = [
                        (t.Index[0] + 3, t.Index[1] + 1),
                        (t.Index[0] - 2, t.Index[1] - 3),
                        (t.Index[0] + 1, t.Index[1] - 2),
                        (t.Index[0] - 1, t.Index[1] + 2),
                        (t.Index[0] + 2, t.Index[1] + 3),
                        (t.Index[0] - 3, t.Index[1] - 1),
                    ]
                    for qi, qj in neighbors:
                        neighbor = z7_dict.get((qi, qj))
                        if neighbor in name_to_index:
                            row_indices.append(name_to_index[cell_id])
                            col_indices.append(name_to_index[neighbor])

                if row_indices:
                    connectivity_matrix = csr_matrix(
                        (np.ones(len(row_indices)), (row_indices, col_indices)),
                        shape=(len(class_df), len(class_df))
                    )
                    num_patches, _ = connected_components(csgraph=connectivity_matrix, directed=False)
                else:
                    # No edges, each cell is isolated â†’ each is a separate patch
                    num_patches = len(class_df)

            area_m2 = cell_area_df.loc[res, 'average_hexagon_area_m2']
            total_area_ha = class_df['Z7_STRING'].nunique() * area_m2 / 10000
            patch_density = num_patches / total_area_ha if total_area_ha > 0 else np.nan

            density_records.append((majority_class, patch_density))

        return dict(density_records)

    final_records = []
    for res in sorted(df['resolution'].unique()):
        df_res = df[df['resolution'] == res]
        flat_density = compute_density(df_res, 'landuse_class_flat', res)
        hier_density = compute_density(df_res, 'landuse_class_hierarchical', res)

        all_codes = set(flat_density.keys()).union(hier_density.keys())
        for code in all_codes:
            final_records.append({
                'resolution': res,
                'code': code,
                'patch_density_flat': round(flat_density.get(code, np.nan), 9),
                'patch_density_hier': round(hier_density.get(code, np.nan), 9),
            })

    return pd.DataFrame(final_records)

"""# **PLADJ**"""

from collections import defaultdict
import numpy as np
import tqdm

def calculate_pladj(df):
    """
    Calculate PLADJ (Percentage of Like Adjacencies) per land use class and resolution
    for both flat and hierarchical land use classifications.

    Parameters:
        df (pd.DataFrame): Must include columns I, J, landuse_class_flat, landuse_class_hierarchical,
                           and index as Z7_STRING

    Returns:
        pd.DataFrame: DataFrame with columns: resolution, code (land use class), pladj_flat, pladj_hier
    """
    all_results = []

    for res in sorted(df['resolution'].unique()):
        df_res = df[df['resolution'] == res]

        def compute_pladj(indexed_df, class_col):
            value_dict = indexed_df[class_col].to_dict()
            z7_dict = indexed_df['Z7_STRING'].to_dict()

            like_adjacencies_count = defaultdict(int)
            total_adjacencies_count = defaultdict(int)

            for t in tqdm.tqdm(indexed_df.itertuples(), desc=f"PLADJ {class_col} res {res}", leave=False):
                cell_id = z7_dict.get(t.Index)
                value_centre = getattr(t, class_col)

                neighbors = [
                    (t.Index[0] + 3, t.Index[1] + 1),
                    (t.Index[0] - 2, t.Index[1] - 3),
                    (t.Index[0] + 1, t.Index[1] - 2),
                    (t.Index[0] - 1, t.Index[1] + 2),
                    (t.Index[0] + 2, t.Index[1] + 3),
                    (t.Index[0] - 3, t.Index[1] - 1),
                ]

                for qi, qj in neighbors:
                    neighbour_class = value_dict.get((qi, qj), np.nan)
                    total_adjacencies_count[value_centre] += 1
                    if value_centre == neighbour_class:
                        like_adjacencies_count[value_centre] += 1

            results = {}
            for patch_type, like_count in like_adjacencies_count.items():
                total_count = total_adjacencies_count[patch_type]
                pladj = (like_count / total_count) * 100 if total_count > 0 else 0
                results[patch_type] = round(pladj, 2)

            return results

        # Create indexed dataframe for both calculations
        base_cols = ['I', 'J', 'landuse_class_flat', 'landuse_class_hierarchical']
        indexed_df = df_res[base_cols].copy()
        indexed_df['Z7_STRING'] = df_res.index
        indexed_df = indexed_df.set_index(['I', 'J'])

        flat_result = compute_pladj(indexed_df, 'landuse_class_flat')
        hier_result = compute_pladj(indexed_df, 'landuse_class_hierarchical')

        all_codes = set(flat_result.keys()).union(hier_result.keys())
        for code in all_codes:
            all_results.append({
                'resolution': res,
                'code': code,
                'pladj_flat': flat_result.get(code, np.nan),
                'pladj_hier': hier_result.get(code, np.nan)
            })

    return pd.DataFrame(all_results)

"""# **SHDI**"""

def calculate_shdi(df):
    grouped_flat = df.groupby(['resolution', 'landuse_class_flat']).size().reset_index(name='count_flat')
    grouped_hier = df.groupby(['resolution', 'landuse_class_hierarchical']).size().reset_index(name='count_hier')

    grouped_flat['area_flat'] = grouped_flat.apply(
        lambda row: row['count_flat'] * cell_area_df.loc[row['resolution'], 'average_hexagon_area_m2'], axis=1
    )
    grouped_hier['area_hier'] = grouped_hier.apply(
        lambda row: row['count_hier'] * cell_area_df.loc[row['resolution'], 'average_hexagon_area_m2'], axis=1
    )

    total_area_flat = grouped_flat.groupby('resolution')['area_flat'].sum().reset_index(name='total_area_flat')
    total_area_hier = grouped_hier.groupby('resolution')['area_hier'].sum().reset_index(name='total_area_hier')

    grouped_flat = pd.merge(grouped_flat, total_area_flat, on='resolution')
    grouped_hier = pd.merge(grouped_hier, total_area_hier, on='resolution')

    grouped_flat['p_i_flat'] = grouped_flat['area_flat'] / grouped_flat['total_area_flat']
    grouped_hier['p_i_hier'] = grouped_hier['area_hier'] / grouped_hier['total_area_hier']

    grouped_flat['shdi_component_flat'] = -grouped_flat['p_i_flat'] * np.log(grouped_flat['p_i_flat'])
    grouped_hier['shdi_component_hier'] = -grouped_hier['p_i_hier'] * np.log(grouped_hier['p_i_hier'])

    shdi_flat = grouped_flat.groupby('resolution')['shdi_component_flat'].sum().reset_index()
    shdi_hier = grouped_hier.groupby('resolution')['shdi_component_hier'].sum().reset_index()

    shdi_df = pd.merge(shdi_flat, shdi_hier, on='resolution')
    shdi_df.columns = ['resolution', 'shdi_flat', 'shdi_hier']
    shdi_df['shdi_flat'] = shdi_df['shdi_flat'].round(4)
    shdi_df['shdi_hier'] = shdi_df['shdi_hier'].round(4)

    return shdi_df

"""# **Workflow for one tile**"""

# Get all resolutions up to 14
all_res_df = dggrid_get_res(dggrid_instance, "ISEA7H", max_res=14)

# Filter for resolution 7 to 14 and keep resolution as index
cell_area_df = all_res_df.loc[7:14].copy()
cell_area_df.index.name = 'isea7h_resolution'

df = pd.read_parquet('/content/test_to_calculate_landscape_metrics.parquet')

# Calculate individual metrics
proportions_df = calculate_class_proportions(df)
patch_density_df = calculate_patch_density(df)
pladj_df = calculate_pladj(df)
shdi_df = calculate_shdi(df)

# Merge all metrics into a single DataFrame
all_metrics = proportions_df.merge(patch_density_df, on=['resolution', 'code'], how='outer')
all_metrics = all_metrics.merge(pladj_df, on=['resolution', 'code'], how='outer')
all_metrics = all_metrics.merge(shdi_df, on='resolution', how='left')

# Save final result as parquet
os.makedirs("output", exist_ok=True)
all_metrics.to_parquet("output/tile_metrics.parquet", compression='snappy')

"""# **Loop for all tiles**"""

import geopandas as gpd
import pandas as pd
from tqdm import tqdm

# Load tile index GPKG containing NR and dataurl columns
tile_index = gpd.read_file(".....gpkg")

# Get all resolutions up to 14
all_res_df = dggrid_get_res(dggrid_instance, "ISEA7H", max_res=14)

# Filter for resolution 7 to 14 and keep resolution as index
cell_area_df = all_res_df.loc[7:14].copy()
cell_area_df.index.name = 'isea7h_resolution'

# Loop over each tile and compute metrics
for idx, row in tqdm(tile_index.iterrows(), total=len(tile_index), desc="Processing tiles"):
    tile_id = row['NR']
    data_url = row['dataurl']

    df = pd.read_parquet(data_url)

    # Skip tile if any res=14 cells have landuse_class == 12
    if ((df['resolution'] == 14) & ((df['landuse_class_flat'] == 12) | (df['landuse_class_hierarchical'] == 12))).any():
        print(f"Skipping tile {tile_id} due to foreign country class (12) at resolution 14")
        continue

    # Calculate individual metrics
    proportions_df = calculate_class_proportions(df)
    patch_density_df = calculate_patch_density(df)
    pladj_df = calculate_pladj(df)
    shdi_df = calculate_shdi(df)

    # Merge all metrics into a single DataFrame
    all_metrics = proportions_df.merge(patch_density_df, on=['resolution', 'code'], how='outer')
    all_metrics = all_metrics.merge(pladj_df, on=['resolution', 'code'], how='outer')
    all_metrics = all_metrics.merge(shdi_df, on='resolution', how='left')

    # Save final result as parquet
    output_path = f"output/tile_{tile_id}_metrics.parquet"
    all_metrics.to_parquet(output_path, compression='snappy')

"""# **Merging landscape metrics of all tiles**"""

import pandas as pd
import requests
from io import BytesIO
from tqdm import tqdm

# Base URL pattern
base_url = "https://storage.googleapis.com/geo-assets/grid_tiles/landscape_dggs/metrics/tile_{}_metrics.parquet"

# Define the range of tile IDs to check
tile_ids = range(4434, 7414)  # Adjust this range based on what you expect

# Store all dataframes
dfs = []

for tile_id in tqdm(tile_ids, desc="Checking tiles"):
    url = base_url.format(tile_id)

    try:
        response = requests.get(url)
        response.raise_for_status()

        tile_df = pd.read_parquet(BytesIO(response.content))
        tile_df["tile_id"] = tile_id
        dfs.append(tile_df)

    except requests.HTTPError as e:
        if e.response.status_code == 404:
            continue  # Skip missing tiles silently
        else:
            print(f"HTTP error on {tile_id}: {e}")
    except Exception as e:
        print(f"Error processing tile {tile_id}: {e}")

# Merge all tiles
merged_df = pd.concat(dfs, ignore_index=True)


# Save if needed
# merged_df.to_parquet("all_merged_tiles.parquet", compression='snappy')

print(merged_df.head())

"""# **Dominant, Cohesive Landscape**"""

import pandas as pd

# 1. Filter only resolution 14
res14 = merged_df[merged_df["resolution"] == 14].copy()

# 2. Drop rows with missing values for relevant metrics
metrics = ["flat_proportion", "patch_density_flat", "pladj_flat"]
res14_clean = res14.dropna(subset=metrics).copy()


# 4. Apply thresholds for Dominant, Cohesive Landscape
dominant_cohesive_tiles = res14_clean[
    (res14_clean["flat_proportion"] >= 75) &
    (res14_clean["patch_density_flat"] <= 10) &
    (res14_clean["pladj_flat"] >= 50)
].copy()

# 5. Show results
print(f"Number of 'Dominant, Cohesive Landscape' tiles: {len(dominant_cohesive_tiles)}")
print(dominant_cohesive_tiles[["tile_id", "code", "flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]])

import pandas as pd

# Keep only tile_id and code combinations from dominant_cohesive_tiles
dominant_cohesive_keys = dominant_cohesive_tiles[['tile_id', 'code']].drop_duplicates()

# Create a new DataFrame with all resolutions from 8 to 14 for each tile_id, code pair
resolutions = list(range(8, 15))
expanded_keys = dominant_cohesive_keys.assign(key=1).merge(
    pd.DataFrame({'resolution': resolutions, 'key': 1}), on='key'
).drop('key', axis=1)

# Merge with merged_df to get values from coarser resolutions
merged_subset_dominant_cohesive = pd.merge(
    expanded_keys,
    merged_df,
    on=['tile_id', 'code', 'resolution'],
    how='left'
)

# Fill NaNs in metric columns with 0, indicating disappearance
metrics_to_fill = ['flat_proportion', 'hier_proportion' , 'patch_density_flat', 'patch_density_hier',
                   'pladj_flat', 'pladj_hier',  'shdi_flat', 'shdi_hier']
for col in metrics_to_fill:
    if col in merged_subset_dominant_cohesive.columns:
        merged_subset_dominant_cohesive[col] = merged_subset_dominant_cohesive[col].fillna(0)

# Re-import necessary libraries after code state reset
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr


# Filter for resolutions 8 to 14 only
filtered_df_1 = df_1[(df_1["resolution"] >= 8) & (df_1["resolution"] <= 14)]

# Define metrics to calculate correlation with resolution (descending)
metrics_flat = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
metrics_hier = ["hier_proportion", "patch_density_hier", "pladj_hier", "shdi_hier"]

# Reverse resolution to descending for correlation calculation
filtered_df_1["inv_resolution"] = 14 - filtered_df_1["resolution"]

# Function to calculate average Pearson correlation per metric
def average_correlation(df, metrics):
    correlations = {}
    for metric in metrics:
        valid_df = df[[metric, "inv_resolution"]].dropna()
        if not valid_df.empty and valid_df[metric].nunique() > 1:
            corr, _ = pearsonr(valid_df["inv_resolution"], valid_df[metric])
            correlations[metric] = round(corr, 2)
        else:
            correlations[metric] = np.nan
    return correlations

# Calculate correlations
flat_corr = average_correlation(filtered_df_1, metrics_flat)
hier_corr = average_correlation(filtered_df_1, metrics_hier)

# Create heatmap-friendly DataFrames
flat_corr_df = pd.DataFrame([flat_corr])
hier_corr_df = pd.DataFrame([hier_corr])

# Rename for display
flat_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']
hier_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']

# Plot heatmaps
fig, axes = plt.subplots(2, 1, figsize=(12, 4))
sns.heatmap(flat_corr_df, annot=True, cmap="coolwarm", center=0, ax=axes[0], cbar=False, annot_kws={'fontsize': 14})
axes[0].set_title("Sensitivity of Metrics to Flat Aggregation", fontsize = 16)

sns.heatmap(hier_corr_df, annot=True, cmap="Greens", center=-1, ax=axes[1], cbar=False, annot_kws={'fontsize': 14})
axes[1].set_title("Sensitivity of Metrics to Hierarchical Aggregation", fontsize = 16)

plt.tight_layout()
plt.show()

# Redefine combined_metrics since the execution state was reset
combined_metrics = {
    ("flat_proportion", "hier_proportion"): "Proportion (%)",
    ("patch_density_flat", "patch_density_hier"): "Patch Density (patches/ha)",
    ("pladj_flat", "pladj_hier"): "PLADJ (%)",
    ("shdi_flat", "shdi_hier"): "SHDI (index)"
}

# Re-run the plotting logic with the defined combined_metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, ((flat_metric, hier_metric), label) in enumerate(combined_metrics.items()):
    ax = axes[i]

    for metric, color, name in [(flat_metric, "blue", "Flat"), (hier_metric, "green", "Hierarchical")]:
        grouped = df_1.groupby("resolution")[metric].agg(["mean", "min", "max"]).dropna()
        x = grouped.index.to_numpy(dtype=float)
        y_mean = grouped["mean"].to_numpy(dtype=float)
        y_min = grouped["min"].to_numpy(dtype=float)
        y_max = grouped["max"].to_numpy(dtype=float)

        ax.plot(x, y_mean, label=f"{name} Mean", color=color)
        ax.fill_between(x, y_min, y_max, color=color, alpha=0.2)

    ax.set_title(label, fontsize=12)
    ax.set_xlabel("Resolution")
    ax.set_ylabel(label)
    ax.invert_xaxis()
    ax.grid(True)
    ax.legend(loc="upper right", fontsize=10)
    ax.set_facecolor("white")
    ax.grid(False)

fig.suptitle("Flat vs Hierarchical Aggregation Metric Trends for Dominant and Cohesive Land Use Class", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""# **Dispersed Fragments**"""

import pandas as pd

# 1. Filter to only resolution 14
res14 = merged_df[merged_df["resolution"] == 14].copy()

# 2. Drop rows with missing values for key metrics
metrics = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
res14_clean = res14.dropna(subset=metrics).copy()


# 4. Apply "Diverse and Scattered" criteria
pd_threshold = 10
shdi_threshold = 0.5
pladj_threshold = 50

diverse_scattered_tiles = res14_clean[
    (res14_clean["flat_proportion"] > 10) &
    (res14_clean["flat_proportion"] < 40) &
    (res14_clean["shdi_flat"] > shdi_threshold) &
    (res14_clean["pladj_flat"] < pladj_threshold)
].copy()

# 5. Output result
print(f"Number of 'Dispersed Fragments' tiles: {len(diverse_scattered_tiles)}")
print(diverse_scattered_tiles[["tile_id", "flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]])

diverse_scattered_tiles

import pandas as pd

# Keep only tile_id and code combinations from diverse_scattered_tiles
diverse_scattered_keys = diverse_scattered_tiles[['tile_id', 'code']].drop_duplicates()

# Create a new DataFrame with all resolutions from 8 to 14 for each tile_id, code pair
resolutions = list(range(8, 15))
expanded_keys = diverse_scattered_keys.assign(key=1).merge(
    pd.DataFrame({'resolution': resolutions, 'key': 1}), on='key'
).drop('key', axis=1)

# Merge with merged_df to get values from coarser resolutions
merged_subset_diverse_scattered = pd.merge(
    expanded_keys,
    merged_df,
    on=['tile_id', 'code', 'resolution'],
    how='left'
)

# Fill NaNs in metric columns with 0, indicating disappearance
metrics_to_fill = ['flat_proportion', 'hier_proportion' , 'patch_density_flat', 'patch_density_hier',
                   'pladj_flat', 'pladj_hier',  'shdi_flat', 'shdi_hier']
for col in metrics_to_fill:
    if col in merged_subset_diverse_scattered.columns:
        merged_subset_diverse_scattered[col] = merged_subset_diverse_scattered[col].fillna(0)

# Re-import necessary libraries after code state reset
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr


# Filter for resolutions 8 to 14 only
filtered_df_2 = df_2[(df_2["resolution"] >= 8) & (df_2["resolution"] <= 14)]

# Define metrics to calculate correlation with resolution (descending)
metrics_flat = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
metrics_hier = ["hier_proportion", "patch_density_hier", "pladj_hier", "shdi_hier"]

# Reverse resolution to descending for correlation calculation
filtered_df_2["inv_resolution"] = 14 - filtered_df_2["resolution"]

# Function to calculate average Pearson correlation per metric
def average_correlation(df, metrics):
    correlations = {}
    for metric in metrics:
        valid_df = df[[metric, "inv_resolution"]].dropna()
        if not valid_df.empty and valid_df[metric].nunique() > 1:
            corr, _ = pearsonr(valid_df["inv_resolution"], valid_df[metric])
            correlations[metric] = round(corr, 2)
        else:
            correlations[metric] = np.nan
    return correlations

# Calculate correlations
flat_corr = average_correlation(filtered_df_2, metrics_flat)
hier_corr = average_correlation(filtered_df_2, metrics_hier)

# Create heatmap-friendly DataFrames
flat_corr_df = pd.DataFrame([flat_corr])
hier_corr_df = pd.DataFrame([hier_corr])

# Rename for display
flat_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']
hier_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']

# Plot heatmaps
fig, axes = plt.subplots(1, 2, figsize=(12, 2))
sns.heatmap(flat_corr_df, annot=True, cmap="coolwarm", center=0, ax=axes[0], cbar=False)
axes[0].set_title("Sensitivity of Metrics to Flat Aggregation")

sns.heatmap(hier_corr_df, annot=True, cmap="Greens", center=-1, ax=axes[1], cbar=False)
axes[1].set_title("Sensitivity of Metrics to Hierarchical Aggregation")

plt.tight_layout()
plt.show()

"""# **Clustered Dispersion (Fragmented but Internally Connected Spatial Pattern)**"""

import pandas as pd

# 1. Filter only resolution 14
res14 = merged_df[merged_df["resolution"] == 14].copy()

# 2. Drop rows with missing values
metrics = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
res14_clean = res14.dropna(subset=metrics).copy()

# 4. Apply criteria for "Diverse, Fragmented, but Cohesive"
shdi_threshold = 0.7
pd_threshold = 10
pladj_threshold = 50

diverse_fragmented_cohesive = res14_clean[
    (res14_clean["flat_proportion"] > 15) &
    (res14_clean["flat_proportion"] < 40) &
    (res14_clean["pladj_flat"] > pladj_threshold)&
    (res14_clean["shdi_flat"] > shdi_threshold)
].copy()


# 5. Output result
print(f"Number of 'Diverse, Fragmented, but Cohesive' tiles: {len(diverse_fragmented_cohesive)}")
print(diverse_fragmented_cohesive[["tile_id", "code", "flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]])

# Re-import necessary libraries after code state reset
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr


# Filter for resolutions 8 to 14 only
filtered_df_3 = df_3[(df_3["resolution"] >= 8) & (df_3["resolution"] <= 14)]

# Define metrics to calculate correlation with resolution (descending)
metrics_flat = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
metrics_hier = ["hier_proportion", "patch_density_hier", "pladj_hier", "shdi_hier"]

# Reverse resolution to descending for correlation calculation
filtered_df_3["inv_resolution"] = 14 - filtered_df_3["resolution"]

# Function to calculate average Pearson correlation per metric
def average_correlation(df, metrics):
    correlations = {}
    for metric in metrics:
        valid_df = df[[metric, "inv_resolution"]].dropna()
        if not valid_df.empty and valid_df[metric].nunique() > 1:
            corr, _ = pearsonr(valid_df["inv_resolution"], valid_df[metric])
            correlations[metric] = round(corr, 2)
        else:
            correlations[metric] = np.nan
    return correlations

# Calculate correlations
flat_corr = average_correlation(filtered_df_3, metrics_flat)
hier_corr = average_correlation(filtered_df_3, metrics_hier)

# Create heatmap-friendly DataFrames
flat_corr_df = pd.DataFrame([flat_corr])
hier_corr_df = pd.DataFrame([hier_corr])

# Rename for display
flat_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']
hier_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']

# Plot heatmaps
fig, axes = plt.subplots(2, 1, figsize=(12, 4))
sns.heatmap(flat_corr_df, annot=True, cmap="coolwarm", center=0, ax=axes[0], cbar=False, annot_kws={'fontsize': 14})
axes[0].set_title("Sensitivity of Metrics to Flat Aggregation", fontsize = 16)

sns.heatmap(hier_corr_df, annot=True, cmap="Greens", center=-1, ax=axes[1], cbar=False, annot_kws={'fontsize': 14})
axes[1].set_title("Sensitivity of Metrics to Hierarchical Aggregation", fontsize = 16)

plt.tight_layout()
plt.show()

# Redefine combined_metrics since the execution state was reset
combined_metrics = {
    ("flat_proportion", "hier_proportion"): "Proportion (%)",
    ("patch_density_flat", "patch_density_hier"): "Patch Density (patches/ha)",
    ("pladj_flat", "pladj_hier"): "PLADJ (%)",
    ("shdi_flat", "shdi_hier"): "SHDI (index)"
}

# Re-run the plotting logic with the defined combined_metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, ((flat_metric, hier_metric), label) in enumerate(combined_metrics.items()):
    ax = axes[i]

    for metric, color, name in [(flat_metric, "blue", "Flat"), (hier_metric, "green", "Hierarchical")]:
        grouped = df_3.groupby("resolution")[metric].agg(["mean", "min", "max"]).dropna()
        x = grouped.index.to_numpy(dtype=float)
        y_mean = grouped["mean"].to_numpy(dtype=float)
        y_min = grouped["min"].to_numpy(dtype=float)
        y_max = grouped["max"].to_numpy(dtype=float)

        ax.plot(x, y_mean, label=f"{name} Mean", color=color)
        ax.fill_between(x, y_min, y_max, color=color, alpha=0.2)

    ax.set_title(label, fontsize=12)
    ax.set_xlabel("Resolution")
    ax.set_ylabel(label)
    ax.invert_xaxis()
    ax.grid(True)
    ax.legend(loc="upper right", fontsize=10)
    ax.set_facecolor("white")
    ax.grid(False)

fig.suptitle("Flat vs Hierarchical Aggregation Metric Trends for Fragmented but Cohesive Land Use Class in Diverse Landscape", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""# **Scattered Minor Class**"""

# 1. Filter only resolution 14
res14 = merged_df[merged_df["resolution"] == 14].copy()

# 2. Drop rows with missing values
metrics = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
res14_clean = res14.dropna(subset=metrics).copy()

scattered_minor_class = res14_clean[
    (res14_clean["flat_proportion"] < 10) &
    (res14_clean["patch_density_flat"] > 10) &
    (res14_clean["pladj_flat"] < 50)
].copy()

print(f"Scattered Minor Class tiles: {len(scattered_minor_class)}")
print(scattered_minor_class[["tile_id", "code", "flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]])

scattered_minor_class

import pandas as pd

# Keep only tile_id and code combinations from scattered_minor_class
scattered_keys = scattered_minor_class[['tile_id', 'code']].drop_duplicates()

# Create a new DataFrame with all resolutions from 8 to 14 for each tile_id, code pair
resolutions = list(range(8, 15))
expanded_keys = scattered_keys.assign(key=1).merge(
    pd.DataFrame({'resolution': resolutions, 'key': 1}), on='key'
).drop('key', axis=1)

# Merge with merged_df to get values from coarser resolutions
merged_subset = pd.merge(
    expanded_keys,
    merged_df,
    on=['tile_id', 'code', 'resolution'],
    how='left'
)

# Fill NaNs in metric columns with 0, indicating disappearance
metrics_to_fill = ['flat_proportion', 'hier_proportion' , 'patch_density_flat', 'patch_density_hier',
                   'pladj_flat', 'pladj_hier',  'shdi_flat', 'shdi_hier']
for col in metrics_to_fill:
    if col in merged_subset.columns:
        merged_subset[col] = merged_subset[col].fillna(0)

# Redefine combined_metrics since the execution state was reset
combined_metrics = {
    ("flat_proportion", "hier_proportion"): "Proportion (%)",
    ("patch_density_flat", "patch_density_hier"): "Patch Density (patches/ha)",
    ("pladj_flat", "pladj_hier"): "PLADJ (%)",
    ("shdi_flat", "shdi_hier"): "SHDI (index)"
}

# Re-run the plotting logic with the defined combined_metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, ((flat_metric, hier_metric), label) in enumerate(combined_metrics.items()):
    ax = axes[i]

    for metric, color, name in [(flat_metric, "blue", "Flat"), (hier_metric, "green", "Hierarchical")]:
        grouped = df_4.groupby("resolution")[metric].agg(["mean", "min", "max"]).dropna()
        x = grouped.index.to_numpy(dtype=float)
        y_mean = grouped["mean"].to_numpy(dtype=float)
        y_min = grouped["min"].to_numpy(dtype=float)
        y_max = grouped["max"].to_numpy(dtype=float)

        ax.plot(x, y_mean, label=f"{name} Mean", color=color)
        ax.fill_between(x, y_min, y_max, color=color, alpha=0.2)

    ax.set_title(label, fontsize=12)
    ax.set_xlabel("Resolution")
    ax.set_ylabel(label)
    ax.invert_xaxis()
    ax.grid(True)
    ax.legend(loc="upper right", fontsize=10)
    ax.set_facecolor("white")
    ax.grid(False)

fig.suptitle("Flat vs Hierarchical Aggregation Metric Trends for Scattered Minor Classes", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

# Re-import necessary libraries after code state reset
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr


# Filter for resolutions 8 to 14 only
filtered_df_4 = df_4[(df_4["resolution"] >= 8) & (df_4["resolution"] <= 14)]

# Define metrics to calculate correlation with resolution (descending)
metrics_flat = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
metrics_hier = ["hier_proportion", "patch_density_hier", "pladj_hier", "shdi_hier"]

# Reverse resolution to descending for correlation calculation
filtered_df_4["inv_resolution"] = 14 - filtered_df_4["resolution"]

# Function to calculate average Pearson correlation per metric
def average_correlation(df, metrics):
    correlations = {}
    for metric in metrics:
        valid_df = df[[metric, "inv_resolution"]].dropna()
        if not valid_df.empty and valid_df[metric].nunique() > 1:
            corr, _ = pearsonr(valid_df["inv_resolution"], valid_df[metric])
            correlations[metric] = round(corr, 2)
        else:
            correlations[metric] = np.nan
    return correlations

# Calculate correlations
flat_corr = average_correlation(filtered_df_4, metrics_flat)
hier_corr = average_correlation(filtered_df_4, metrics_hier)

# Create heatmap-friendly DataFrames
flat_corr_df = pd.DataFrame([flat_corr])
hier_corr_df = pd.DataFrame([hier_corr])

# Rename for display
flat_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']
hier_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']

# Plot heatmaps
fig, axes = plt.subplots(2, 1, figsize=(12, 4))
sns.heatmap(flat_corr_df, annot=True, cmap="coolwarm", center=0, ax=axes[0], cbar=False, annot_kws={'fontsize': 14})
axes[0].set_title("Sensitivity of Metrics to Flat Aggregation", fontsize = 16)

sns.heatmap(hier_corr_df, annot=True, cmap="Greens", center=0, ax=axes[1], cbar=False, annot_kws={'fontsize': 14})
axes[1].set_title("Sensitivity of Metrics to Hierarchical Aggregation", fontsize = 16)

plt.tight_layout()
plt.show()

"""# **Connected Minor Class**"""

# 1. Filter only resolution 14
res14 = merged_df[merged_df["resolution"] == 14].copy()

# 2. Drop rows with missing values
metrics = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
res14_clean = res14.dropna(subset=metrics).copy()

connected_minor_class = res14_clean[
    (res14_clean["flat_proportion"] < 10) &
    (res14_clean["patch_density_flat"] < 5) &
    (res14_clean["pladj_flat"] > 70)
].copy()

#print(f"Connected Minor Class tiles: {len(connected_minor_class)}")
##print(connected_minor_class[["tile_id", "flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]])

import pandas as pd

# Keep only tile_id and code combinations from scattered_minor_class
connected_minor_keys = connected_minor_class[['tile_id', 'code']].drop_duplicates()

# Create a new DataFrame with all resolutions from 8 to 14 for each tile_id, code pair
resolutions = list(range(8, 15))
expanded_keys = connected_minor_keys.assign(key=1).merge(
    pd.DataFrame({'resolution': resolutions, 'key': 1}), on='key'
).drop('key', axis=1)

# Merge with merged_df to get values from coarser resolutions
merged_subset_connected_minor = pd.merge(
    expanded_keys,
    merged_df,
    on=['tile_id', 'code', 'resolution'],
    how='left'
)

# Fill NaNs in metric columns with 0, indicating disappearance
metrics_to_fill = ['flat_proportion', 'hier_proportion' , 'patch_density_flat', 'patch_density_hier',
                   'pladj_flat', 'pladj_hier',  'shdi_flat', 'shdi_hier']
for col in metrics_to_fill:
    if col in merged_subset_connected_minor.columns:
        merged_subset_connected_minor[col] = merged_subset_connected_minor[col].fillna(0)

#import ace_tools as tools; tools.display_dataframe_to_user(name="Expanded Scattered Minor Class", dataframe=merged_subset)

# Re-import necessary libraries after code state reset
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr


# Filter for resolutions 8 to 14 only
filtered_df_5 = df_5[(df_5["resolution"] >= 8) & (df_5["resolution"] <= 14)]

# Define metrics to calculate correlation with resolution (descending)
metrics_flat = ["flat_proportion", "patch_density_flat", "pladj_flat", "shdi_flat"]
metrics_hier = ["hier_proportion", "patch_density_hier", "pladj_hier", "shdi_hier"]

# Reverse resolution to descending for correlation calculation
filtered_df_5["inv_resolution"] = 14 - filtered_df_5["resolution"]

# Function to calculate average Pearson correlation per metric
def average_correlation(df, metrics):
    correlations = {}
    for metric in metrics:
        valid_df = df[[metric, "inv_resolution"]].dropna()
        if not valid_df.empty and valid_df[metric].nunique() > 1:
            corr, _ = pearsonr(valid_df["inv_resolution"], valid_df[metric])
            correlations[metric] = round(corr, 2)
        else:
            correlations[metric] = np.nan
    return correlations

# Calculate correlations
flat_corr = average_correlation(filtered_df_5, metrics_flat)
hier_corr = average_correlation(filtered_df_5, metrics_hier)

# Create heatmap-friendly DataFrames
flat_corr_df = pd.DataFrame([flat_corr])
hier_corr_df = pd.DataFrame([hier_corr])

# Rename for display
flat_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']
hier_corr_df.columns = ['Proportion (%)', 'PD (patch/ha)', 'PLADJ (%)', 'SHDI']

# Plot heatmaps
fig, axes = plt.subplots(2, 1, figsize=(12, 4))
sns.heatmap(flat_corr_df, annot=True, cmap="coolwarm", center=0, ax=axes[0], cbar=False, annot_kws={'fontsize': 14})
axes[0].set_title("Sensitivity of Metrics to Flat Aggregation", fontsize=16)

sns.heatmap(hier_corr_df, annot=True, cmap="Greens", center=0, ax=axes[1], cbar=False, annot_kws={'fontsize': 14})
axes[1].set_title("Sensitivity of Metrics to Hierarchical Aggregation", fontsize=16)

plt.tight_layout()
plt.show()

# Redefine combined_metrics since the execution state was reset
combined_metrics = {
    ("flat_proportion", "hier_proportion"): "Proportion (%)",
    ("patch_density_flat", "patch_density_hier"): "Patch Density (patches/ha)",
    ("pladj_flat", "pladj_hier"): "PLADJ (%)",
    ("shdi_flat", "shdi_hier"): "SHDI (index)"
}

# Re-run the plotting logic with the defined combined_metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, ((flat_metric, hier_metric), label) in enumerate(combined_metrics.items()):
    ax = axes[i]

    for metric, color, name in [(flat_metric, "blue", "Flat"), (hier_metric, "green", "Hierarchical")]:
        grouped = df_5.groupby("resolution")[metric].agg(["mean", "min", "max"]).dropna()
        x = grouped.index.to_numpy(dtype=float)
        y_mean = grouped["mean"].to_numpy(dtype=float)
        y_min = grouped["min"].to_numpy(dtype=float)
        y_max = grouped["max"].to_numpy(dtype=float)

        ax.plot(x, y_mean, label=f"{name} Mean", color=color)
        ax.fill_between(x, y_min, y_max, color=color, alpha=0.2)

    ax.set_title(label, fontsize=16)
    ax.set_xlabel("Resolution")
    ax.set_ylabel(label)
    ax.invert_xaxis()
    ax.grid(True)
    ax.legend(loc="upper right", fontsize=13)
    ax.set_facecolor("white")
    ax.grid(False)

fig.suptitle("Flat vs Hierarchical Aggregation Metric Trends for Connected Minor Class", fontsize=20)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""# **The whole Estonia**"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr



# Filter only resolution 8 to 14
df_full = merged_df[merged_df["resolution"] > 7].copy()
df_full["inv_resolution"] = 14 - df_full["resolution"]

# Define metric labels
flat_metrics = {
    "flat_proportion": "Proportion (%)",
    "patch_density_flat": "PD (patches/ha)",
    "pladj_flat": "PLADJ (%)",
    "shdi_flat": "SHDI"
}
hier_metrics = {
    "hier_proportion": "Proportion (%)",
    "patch_density_hier": "PD (patches/ha)",
    "pladj_hier": "PLADJ (%)",
    "shdi_hier": "SHDI"
}

def average_correlation(df, metrics):
    correlations = {}
    for metric in metrics:
        valid_df = df[[metric, "inv_resolution"]].dropna()
        if not valid_df.empty and valid_df[metric].nunique() > 1:
            corr, _ = pearsonr(valid_df["inv_resolution"], valid_df[metric])
            correlations[metric] = round(corr, 2)
        else:
            correlations[metric] = np.nan
    return correlations

flat_corr = average_correlation(df_full, list(flat_metrics.keys()))
hier_corr = average_correlation(df_full, list(hier_metrics.keys()))

flat_corr_df = pd.DataFrame([flat_corr])
hier_corr_df = pd.DataFrame([hier_corr])
flat_corr_df.columns = list(flat_metrics.values())
hier_corr_df.columns = list(hier_metrics.values())

# --- Heatmaps vertically stacked ---
fig, axes = plt.subplots(2, 1, figsize=(12, 4))

sns.heatmap(flat_corr_df, annot=True, cmap="coolwarm", center=0, ax=axes[0], cbar=False, annot_kws={'fontsize': 14})
axes[0].set_title("Sensitivity of Metrics to Flat Aggregation", fontsize = 16)

sns.heatmap(hier_corr_df, annot=True, cmap="Greens", center=0, ax=axes[1], cbar=False, annot_kws={'fontsize': 14})
axes[1].set_title("Sensitivity of Metrics to Hierarchical Aggregation", fontsize = 16)

plt.tight_layout()
plt.show()

# Redefine combined_metrics since the execution state was reset
combined_metrics = {
    ("flat_proportion", "hier_proportion"): "Proportion (%)",
    ("patch_density_flat", "patch_density_hier"): "Patch Density (patches/ha)",
    ("pladj_flat", "pladj_hier"): "PLADJ (%)",
    ("shdi_flat", "shdi_hier"): "SHDI (index)"
}

# Re-run the plotting logic with the defined combined_metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, ((flat_metric, hier_metric), label) in enumerate(combined_metrics.items()):
    ax = axes[i]

    for metric, color, name in [(flat_metric, "blue", "Flat"), (hier_metric, "green", "Hierarchical")]:
        grouped = df_full.groupby("resolution")[metric].agg(["mean", "min", "max"]).dropna()
        x = grouped.index.to_numpy(dtype=float)
        y_mean = grouped["mean"].to_numpy(dtype=float)
        y_min = grouped["min"].to_numpy(dtype=float)
        y_max = grouped["max"].to_numpy(dtype=float)

        ax.plot(x, y_mean, label=f"{name} Mean", color=color)
        ax.fill_between(x, y_min, y_max, color=color, alpha=0.2)

    ax.set_title(label, fontsize=12)
    ax.set_xlabel("Resolution")
    ax.set_ylabel(label)
    ax.invert_xaxis()
    ax.grid(True)
    ax.legend(loc="upper right", fontsize=10)
    ax.set_facecolor("white")
    ax.grid(False)

fig.suptitle("Flat vs Hierarchical Aggregation Metric Trends (All of Estonia)", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()